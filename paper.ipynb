{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Задачи, цели работы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Цель работы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Цель этой лабораторной работы - изучить и применить готовые библиотеки для оптимизации на Python, в частности, PyTorch и различные методы оптимизации из SciPy. Также, требуется сравнить эффективность работы этих методов с нашими реализациями из прошлых работ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Задачи для достижения указанной цели\n",
    "\n",
    "1. Изучить использование вариантов SGD (torch.optim) из PyTorch. Исследовать их эффективность и сравнить с собственными реализациями из предыдущих работ.\n",
    "2. Изучить использование готовых методов оптимизации из SciPy (scipy.optimize.minimize, scipy.optimize.least_squares). Исследовать их эффективность и сравнить с собственными реализациями из предыдущих работ.\n",
    "3. Реализовать использование PyTorch для вычисления градиента и сравнить его с другими подходами.\n",
    "4. Исследовать, как задание границ изменения параметров влияет на работу методов из SciPy.\n",
    "5. Исследовать использование линейных и нелинейных ограничений при использовании scipy.optimize.minimize из SciPy. Рассмотреть случаи, когда минимум находится на границе заданной области и когда он расположен внутри.\n",
    "6. Подготовить отчет, содержащий описание реализованных алгоритмов, реализацию, необходимые тесты и таблицы. Отчет должен также включать анализ результатов, преимуществ и ограничений методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ход работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Подготовка среды, определение полезных функций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### В предыдущих сериях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import profiler\n",
    "import descent\n",
    "import regression\n",
    "import visualization\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "np.set_printoptions(precision=2, suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "В качестве задачи Least Squares мы будем использовать модель нелинейной регрессии из прошлой работы на базе датасета изменения погоды от Яндекса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.get_data()\n",
    "data_weather = dataset.get_data_weather()\n",
    "visualization.visualize_regression([0.0], *data_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Как видим, данные вполне периодичны. Попробуем их описать синусом:\n",
    "\n",
    "Определим модель как\n",
    "$$M(x, W) = W_1 + W_2 \\cdot x + W_3 \\cdot x^2 + W_4 \\cdot x^3 + W_5 \\cdot \\sin (x \\cdot W_6 + W_7)$$\n",
    "Тогда Loss-функция будет выглядеть как\n",
    "$$\n",
    "f(W) = \\sum_{i\\in{DATA_x}} (DATA_{yi} - M(i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_weather, f_weather_chunk = dataset.get_nonlinear_loss_func(*data_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "В качестве квадратичной задачи для градиентного спуска, будем использовать Loss-функцию линейной регрессии из работы №2. Она построена на базе датасета успеваемости студентов в зависимости от количества часов подготовки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_stud, f_stud_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.visualize_regression([0.0], *data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Она пригодится для сравнения работы различных алгоритмов оптимизации\n",
    "\n",
    "Определим её следующим образом:\n",
    "$$\n",
    "f(x, y) = (1 - x)^2 + 100(y - x^2)^2\n",
    "$$\n",
    "Это нелинейная функция, будем использовать её минимизировать для демонстрации работы некоторых методов в будущем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Задание 1. Исследование реализации SGD из Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Наша реализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "В прошлых работах были реализованы методы:\n",
    "- Sgd\n",
    "- Sgd with momentum\n",
    "- Rmsprop\n",
    "- Adagrad\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descent import (\n",
    "    minibatch_descent,\n",
    "    momentum_minibatch_descent,\n",
    "    rmsprop_minibatch_descent,\n",
    "    adagrad_minibatch_descent,\n",
    "    adam_minibatch_descent,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Функция Розенброка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import squared, f_rosenbrock, f_rosenbrock_chunk\n",
    "\n",
    "print(\"Глобальный минимум: [1.0, 1.0]\")\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"DIY Adam\": adam_minibatch_descent(\n",
    "            f=squared(f_rosenbrock_chunk()),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            decay=descent.step_decay(1.0, 0.8, 100),\n",
    "            n_epochs=1000,\n",
    "            batch_size=1,\n",
    "            tol=0.01,\n",
    "            betta1=0.9,\n",
    "            betta2=0.9,\n",
    "        ),\n",
    "        \"DIY SGD\": minibatch_descent(\n",
    "            f=squared(f_rosenbrock_chunk()),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.constant_lr_decay(0.00001),\n",
    "            n_epochs=1000,\n",
    "            batch_size=1,\n",
    "            tol=0.01,\n",
    "        ),\n",
    "        \"DIY Momentum SGD\": momentum_minibatch_descent(\n",
    "            f=squared(f_rosenbrock_chunk()),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.constant_lr_decay(0.0001),\n",
    "            n_epochs=1000,\n",
    "            batch_size=1,\n",
    "            tol=0.01,\n",
    "            alpha=0.9,\n",
    "        ),\n",
    "        \"DIY RMSProp\": rmsprop_minibatch_descent(\n",
    "            f=squared(f_rosenbrock_chunk()),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.step_decay(1.0, 0.5, 100),\n",
    "            n_epochs=1000,\n",
    "            batch_size=1,\n",
    "            tol=0.01,\n",
    "            alpha=0.9,\n",
    "        ),\n",
    "        \"DIY Adagrad\": adagrad_minibatch_descent(\n",
    "            f=squared(f_rosenbrock_chunk()),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.constant_lr_decay(1.0),\n",
    "            n_epochs=1000,\n",
    "            batch_size=1,\n",
    "            tol=0.01,\n",
    "        ),\n",
    "    },\n",
    "    f_rosenbrock,\n",
    "    print_points=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ввиду обусловленности представленной функции, обычный стохастический спуск так и не удалось адекватно стабилизировать, он начал сходиться только при чрезвычайно маленьком Learning Rate (1e-5). Каждое его улучшение помогает улучшить и ускорить сходимость, что отлично видно на графиках, учитывая подобранный Learning Rate для каждого из методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import squared\n",
    "\n",
    "visualization.linear_multiple_demo_2args(\n",
    "    {\n",
    "        \"DIY Adam\": adam_minibatch_descent(\n",
    "            f=squared(f_stud_chunk),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            decay=descent.step_decay(1.0, 0.8, 100),\n",
    "            n_epochs=1000,\n",
    "            batch_size=1,\n",
    "            tol=0.01,\n",
    "            betta1=0.9,\n",
    "            betta2=0.9,\n",
    "        ),\n",
    "        \"DIY Adagrad\": adagrad_minibatch_descent(\n",
    "            f=squared(f_stud_chunk),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.constant_lr_decay(1.0),\n",
    "            n_epochs=1000,\n",
    "            batch_size=1,\n",
    "            tol=0.01,\n",
    "        ),\n",
    "        \"DIY SGD\": minibatch_descent(\n",
    "            f=squared(f_stud_chunk),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.constant_lr_decay(0.00001),\n",
    "            n_epochs=1000,\n",
    "            batch_size=1,\n",
    "            tol=0.01,\n",
    "        ),\n",
    "        \"DIY Momentum SGD\": momentum_minibatch_descent(\n",
    "            f=squared(f_stud_chunk),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.constant_lr_decay(0.0001),\n",
    "            n_epochs=1000,\n",
    "            batch_size=1,\n",
    "            tol=0.01,\n",
    "            alpha=0.9,\n",
    "        ),\n",
    "        \"DIY RMSProp\": rmsprop_minibatch_descent(\n",
    "            f=squared(f_stud_chunk),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.step_decay(1.0, 0.5, 100),\n",
    "            n_epochs=1000,\n",
    "            batch_size=1,\n",
    "            tol=0.01,\n",
    "            alpha=0.9,\n",
    "        ),\n",
    "    },\n",
    "    f_stud,\n",
    "    *data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Подготовим аналогичные методы в реализации PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Функция Розенброка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "// Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Задание 2. Исследование методов оптимизации SciPy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Сравнение с прошлой работой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### scipy.optimize.least_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Вычисление градиента с помощью PyTorch, сравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### scipy.optimize.least_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Изменение параметров и границ, сравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### scipy.optimize.least_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "// Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Бонусное задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Задание 1. Исследование использования ограничений SciPy при работе с scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### scipy.optimize.LinearConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "// :NOTE: Рассмотреть случаи когда минимум находится на границе заданной области и когда он расположен внутри"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### scipy.optimize.NonlinearConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "// :NOTE: Рассмотреть случаи когда минимум находится на границе заданной области и когда он расположен внутри"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "// Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Заключение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "// вывод"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
