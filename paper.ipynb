{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задачи, цели работы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Цель работы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель этой лабораторной работы - изучить и применить готовые библиотеки для оптимизации на Python, в частности, PyTorch и различные методы оптимизации из SciPy. Также, требуется сравнить эффективность работы этих методов с нашими реализациями из прошлых работ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для достижения указанной цели\n",
    "\n",
    "1. Изучить использование вариантов SGD (torch.optim) из PyTorch. Исследовать их эффективность и сравнить с собственными реализациями из предыдущих работ.\n",
    "2. Изучить использование готовых методов оптимизации из SciPy (scipy.optimize.minimize, scipy.optimize.least_squares). Исследовать их эффективность и сравнить с собственными реализациями из предыдущих работ.\n",
    "3. Реализовать использование PyTorch для вычисления градиента и сравнить его с другими подходами.\n",
    "4. Исследовать, как задание границ изменения параметров влияет на работу методов из SciPy.\n",
    "5. Исследовать использование линейных и нелинейных ограничений при использовании scipy.optimize.minimize из SciPy. Рассмотреть случаи, когда минимум находится на границе заданной области и когда он расположен внутри.\n",
    "6. Подготовить отчет, содержащий описание реализованных алгоритмов, реализацию, необходимые тесты и таблицы. Отчет должен также включать анализ результатов, преимуществ и ограничений методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ход работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка среды, определение полезных функций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В предыдущих сериях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import profiler\n",
    "import descent\n",
    "import regression\n",
    "import visualization\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "np.set_printoptions(precision=2, suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве задачи Least Squares мы будем использовать модель нелинейной регрессии из прошлой работы на базе датасета изменения погоды от Яндекса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.get_data()\n",
    "data_weather = dataset.get_data_weather()\n",
    "visualization.visualize_regression([0.0], *data_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, данные вполне периодичны. Попробуем их описать синусом:\n",
    "\n",
    "Определим модель как\n",
    "$$M(x, W) = W_1 + W_2 \\cdot x + W_3 \\cdot x^2 + W_4 \\cdot x^3 + W_5 \\cdot \\sin (x \\cdot W_6 + W_7)$$\n",
    "Тогда Loss-функция будет выглядеть как\n",
    "$$\n",
    "f(W) = \\sum_{i\\in{DATA_x}} (DATA_{yi} - M(i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_weather, f_weather_chunk = dataset.get_nonlinear_loss_func(*data_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве квадратичной задачи для градиентного спуска, будем использовать Loss-функцию линейной регрессии из работы №2. Она построена на базе датасета успеваемости студентов в зависимости от количества часов подготовки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_stud, f_stud_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.visualize_regression([0.0], *data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Она пригодится для сравнения работы различных алгоритмов оптимизации\n",
    "\n",
    "Определим её следующим образом:\n",
    "$$\n",
    "f(x, y) = (1 - x)^2 + 100(y - x^2)^2\n",
    "$$\n",
    "Это нелинейная функция, будем использовать её минимизировать для демонстрации работы некоторых методов в будущем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Исследование реализации SGD из Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В прошлых работах были реализованы методы:\n",
    "- Sgd\n",
    "- Sgd with momentum\n",
    "- Rmsprop\n",
    "- Adagrad\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from descent import (\n",
    "    minibatch_descent,\n",
    "    nesterov_minibatch_descent,\n",
    "    rmsprop_minibatch_descent,\n",
    "    adagrad_minibatch_descent,\n",
    "    adam_minibatch_descent,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция Розенброка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Наша реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import squared, f_rosenbrock, f_rosenbrock_chunk\n",
    "\n",
    "print(\"Глобальный минимум: [1.0, 1.0]\")\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"Adam\": adam_minibatch_descent(\n",
    "            f=squared(f_rosenbrock_chunk()),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            decay=descent.step_decay(1.0, 0.8, 100),\n",
    "            n_epochs=1000,\n",
    "            tol=0.01,\n",
    "            betta1=0.9,\n",
    "            betta2=0.9,\n",
    "        ),\n",
    "        \"SGD\": minibatch_descent(\n",
    "            f=squared(f_rosenbrock_chunk()),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.constant_lr_decay(0.00001),\n",
    "            n_epochs=1000,\n",
    "            tol=0.01,\n",
    "        ),\n",
    "        \"Nesterov\": nesterov_minibatch_descent(\n",
    "            f=squared(f_rosenbrock_chunk()),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.constant_lr_decay(0.0001),\n",
    "            n_epochs=1000,\n",
    "            tol=0.01,\n",
    "            alpha=0.9,\n",
    "        ),\n",
    "        \"RMSProp\": rmsprop_minibatch_descent(\n",
    "            f=squared(f_rosenbrock_chunk()),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.step_decay(1.0, 0.5, 100),\n",
    "            n_epochs=1000,\n",
    "            tol=0.01,\n",
    "            alpha=0.9,\n",
    "        ),\n",
    "        \"Adagrad\": adagrad_minibatch_descent(\n",
    "            f=squared(f_rosenbrock_chunk()),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.constant_lr_decay(1.0),\n",
    "            n_epochs=1000,\n",
    "            tol=0.01,\n",
    "        ),\n",
    "    },\n",
    "    f_rosenbrock,\n",
    "    print_points=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим аналогичные методы в реализации PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import descent\n",
    "\n",
    "visualization.visualize_multiple_descent_2args(\n",
    "    {\n",
    "        \"Adam\": descent.torch_descent(\n",
    "            torch.optim.Adam(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True)],\n",
    "                lr=1.0,\n",
    "                betas=(0.9, 0.9),\n",
    "            ),\n",
    "            f_rosenbrock,\n",
    "            1000,\n",
    "            torch.optim.lr_scheduler.StepLR,\n",
    "            100,  # Step size\n",
    "            0.8,  # Gamma\n",
    "        ),\n",
    "        \"SGD\": descent.torch_descent(\n",
    "            torch.optim.SGD(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True)],\n",
    "                lr=0.00001,\n",
    "            ),\n",
    "            f_rosenbrock,\n",
    "            1000,\n",
    "            torch.optim.lr_scheduler.ConstantLR,\n",
    "        ),\n",
    "        \"Nesterov\": descent.torch_descent(\n",
    "            torch.optim.SGD(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True)],\n",
    "                lr=0.0001,\n",
    "                momentum=0.1,\n",
    "                nesterov=True,\n",
    "            ),\n",
    "            f_rosenbrock,\n",
    "            1000,\n",
    "            torch.optim.lr_scheduler.ConstantLR,\n",
    "        ),\n",
    "        \"RMSProp\": descent.torch_descent(\n",
    "            torch.optim.RMSprop(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True)],\n",
    "                lr=1.0,\n",
    "                alpha=0.9,\n",
    "            ),\n",
    "            f_rosenbrock,\n",
    "            1000,\n",
    "            torch.optim.lr_scheduler.StepLR,\n",
    "            100,  # Step size\n",
    "            0.5,  # Gamma\n",
    "        ),\n",
    "        \"Adagrad\": descent.torch_descent(\n",
    "            torch.optim.Adagrad(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True)],\n",
    "                lr=1.0,\n",
    "            ),\n",
    "            f_rosenbrock,\n",
    "            1000,\n",
    "            torch.optim.lr_scheduler.ConstantLR,\n",
    "        ),\n",
    "    },\n",
    "    f_rosenbrock,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ввиду обусловленности представленной функции, обычный стохастический спуск так и не удалось адекватно стабилизировать, он начал сходиться только при чрезвычайно маленьком Learning Rate (1e-5). Каждое его улучшение помогает улучшить и ускорить сходимость, что отлично видно на графиках, учитывая подобранный Learning Rate для каждого из методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Наша реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import squared\n",
    "import time\n",
    "\n",
    "calc_start = time.time()\n",
    "visualization.linear_multiple_demo_2args(\n",
    "    {\n",
    "        \"Adam\": adam_minibatch_descent(\n",
    "            f=squared(f_stud_chunk),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            decay=descent.constant_lr_decay(1e-3),\n",
    "            n_epochs=1000,\n",
    "            tol=1e-7,\n",
    "            betta1=0.9,\n",
    "            betta2=0.999,\n",
    "            batch_size=2,\n",
    "        ),\n",
    "        \"Nesterov\": nesterov_minibatch_descent(\n",
    "            f=squared(f_stud_chunk),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.step_decay(0.01, 0.5, 10),\n",
    "            n_epochs=1000,\n",
    "            tol=1e-7,\n",
    "            alpha=0.35,\n",
    "            batch_size=2,\n",
    "        ),\n",
    "        \"RMSProp\": rmsprop_minibatch_descent(\n",
    "            f=squared(f_stud_chunk),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.constant_lr_decay(0.01),\n",
    "            n_epochs=1000,\n",
    "            tol=1e-7,\n",
    "            alpha=0.99,\n",
    "            batch_size=2,\n",
    "        ),\n",
    "        \"Adagrad\": adagrad_minibatch_descent(\n",
    "            f=squared(f_stud_chunk),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.constant_lr_decay(1.0),\n",
    "            n_epochs=1000,\n",
    "            tol=1e-7,\n",
    "            batch_size=2,\n",
    "        ),\n",
    "        \"SGD\": minibatch_descent(\n",
    "            f=squared(f_stud_chunk),\n",
    "            df=descent.numeric_gradient,\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            lr=descent.step_decay(0.01, 0.5, 10),\n",
    "            n_epochs=1000,\n",
    "            tol=1e-7,\n",
    "            batch_size=2,\n",
    "        ),\n",
    "    },\n",
    "    f_stud,\n",
    "    *data\n",
    ")\n",
    "print(\"Elapsed: \", time.time() - calc_start, \"sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch существуют абстракции для построения лосс-функций из датасетов. Так, мы можем воспользоваться `torch.util.data.DataSet` для представления нашего датасета \"про студентов\" в формате PyTorch.\n",
    "\n",
    "Далее, мы можем воспользоваться `torch.util.data.DataLoader` для разбиения нашего датасета на батчи, из которых, в свою очередь, мы будем формировать Loss-функцию на каждой итерации в процессе оптимизации.\n",
    "\n",
    "Так, мы сделаем наш `torch.optim.SGD` по-настоящему SGD. Как ни странно, в самой реализации от PyTorch у SGD нет ничего стохастического и разбиение функции на батчи - это целиком наша задача."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcs import torch_loss\n",
    "from torch.utils.data import DataLoader\n",
    "import dataset\n",
    "\n",
    "y = torch.tensor(data[1], dtype=torch.float)\n",
    "x = torch.tensor(data[0], dtype=torch.float)\n",
    "data_set = dataset.TorchDataset(x, y)\n",
    "\n",
    "torch_loss(x, y)(\n",
    "    torch.tensor([3.247829835435748, 5.313336982957038])\n",
    ")  # Eventually, data_set will be consumed by DataLoader\n",
    "# that will be able to split it into batches in the same \"x, y\" format that can be consumed by \"torch_loss\" loss-function factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_start = time.time()\n",
    "visualization.linear_multiple_demo_2args(\n",
    "    {\n",
    "        \"Adam\": descent.torch_descent_stochastic(\n",
    "            torch.optim.Adam(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True)],\n",
    "                lr=1e-3,\n",
    "                betas=(0.9, 0.999),\n",
    "            ),\n",
    "            torch_loss,\n",
    "            DataLoader(dataset=data_set, batch_size=2),\n",
    "            1000,\n",
    "            torch.optim.lr_scheduler.ConstantLR,\n",
    "        ),\n",
    "        \"Nesterov\": descent.torch_descent_stochastic(\n",
    "            torch.optim.SGD(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True)],\n",
    "                lr=0.01,\n",
    "                nesterov=True,\n",
    "                momentum=1 - 0.35,\n",
    "            ),\n",
    "            torch_loss,\n",
    "            DataLoader(dataset=data_set, batch_size=2),\n",
    "            1000,\n",
    "            torch.optim.lr_scheduler.StepLR,\n",
    "            10,  # Step size\n",
    "            0.5,  # Gamma\n",
    "        ),\n",
    "        \"RMSProp\": descent.torch_descent_stochastic(\n",
    "            torch.optim.RMSprop(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True)],\n",
    "                lr=0.01,\n",
    "                alpha=0.99,\n",
    "            ),\n",
    "            torch_loss,\n",
    "            DataLoader(dataset=data_set, batch_size=2),\n",
    "            1000,\n",
    "            torch.optim.lr_scheduler.ConstantLR,\n",
    "        ),\n",
    "        \"Adagrad\": descent.torch_descent_stochastic(\n",
    "            torch.optim.Adagrad(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True)],\n",
    "                lr=1.0,\n",
    "            ),\n",
    "            torch_loss,\n",
    "            DataLoader(dataset=data_set, batch_size=2),\n",
    "            1000,\n",
    "            torch.optim.lr_scheduler.ConstantLR,\n",
    "        ),\n",
    "        \"SGD\": descent.torch_descent_stochastic(\n",
    "            torch.optim.SGD(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True)],\n",
    "                lr=0.01,\n",
    "            ),\n",
    "            torch_loss,\n",
    "            DataLoader(dataset=data_set, batch_size=2),\n",
    "            1000,\n",
    "            torch.optim.lr_scheduler.StepLR,\n",
    "            10,  # Step size\n",
    "            0.5,  # Gamma\n",
    "        ),\n",
    "    },\n",
    "    f_stud,\n",
    "    *data\n",
    ")\n",
    "print(\"Elapsed: \", time.time() - calc_start, \"sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, судя по графикам, результаты работы алгоритмов чрезвычайно похожи. Однако, реализация PyTorch работала всего 43.563 секунды против 73.169 у нашей реализации.\n",
    "\n",
    "Ускорение в 1.68 раза, более точное вычисление производных благодаря дуальным числам - это ещё не всё, что может дать нам PyTorch. Покажем его истинную силу:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да-да, настоящая сила PyTorch сокрыта в возможности отправить вычисления прямо на нашу видеокарту. Добиться этого нетрудно, достаточно\n",
    "правильно скомпилировать библиотеку, установить нужные CUDA рантаймы, иметь поддерживаемую карту, а так же проинициализировать всё правильно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что `cuda.is_available() == True`, а значит, мы можем двигаться дальше. Осталось просто добавить нашим тензорам параметр `device=cuda` и \"дело в шляпе\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda:0\")\n",
    "torch.cuda.get_device_name(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cuda = x.to(cuda)\n",
    "y_cuda = y.to(cuda)\n",
    "data_set_cuda = dataset.TorchDataset(x_cuda, y_cuda)\n",
    "\n",
    "torch_loss(x_cuda, y_cuda)(\n",
    "    torch.tensor([3.247829835435748, 5.313336982957038], device=cuda)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "calc_start = time.time()\n",
    "visualization.linear_multiple_demo_2args(\n",
    "    {\n",
    "        \"Adam\": descent.torch_descent_stochastic(\n",
    "            torch.optim.Adam(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True, device=cuda)],\n",
    "                lr=5e-2,\n",
    "                betas=(0.9, 0.999),\n",
    "            ),\n",
    "            torch_loss,\n",
    "            DataLoader(dataset=data_set_cuda, batch_size=100),\n",
    "            10000,\n",
    "            torch.optim.lr_scheduler.ConstantLR,\n",
    "        ),\n",
    "        \"Nesterov\": descent.torch_descent_stochastic(\n",
    "            torch.optim.SGD(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True, device=cuda)],\n",
    "                lr=0.01,\n",
    "                nesterov=True,\n",
    "                momentum=1 - 0.35,\n",
    "            ),\n",
    "            torch_loss,\n",
    "            DataLoader(dataset=data_set_cuda, batch_size=100),\n",
    "            10000,\n",
    "            torch.optim.lr_scheduler.StepLR,\n",
    "            500,  # Step size\n",
    "            0.5,  # Gamma\n",
    "        ),\n",
    "        \"RMSProp\": descent.torch_descent_stochastic(\n",
    "            torch.optim.RMSprop(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True, device=cuda)],\n",
    "                lr=0.01,\n",
    "                alpha=0.99,\n",
    "            ),\n",
    "            torch_loss,\n",
    "            DataLoader(dataset=data_set_cuda, batch_size=100),\n",
    "            10000,\n",
    "            torch.optim.lr_scheduler.ConstantLR,\n",
    "        ),\n",
    "        \"Adagrad\": descent.torch_descent_stochastic(\n",
    "            torch.optim.Adagrad(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True, device=cuda)],\n",
    "                lr=1.0,\n",
    "            ),\n",
    "            torch_loss,\n",
    "            DataLoader(dataset=data_set_cuda, batch_size=100),\n",
    "            10000,\n",
    "            torch.optim.lr_scheduler.ConstantLR,\n",
    "        ),\n",
    "        \"SGD\": descent.torch_descent_stochastic(\n",
    "            torch.optim.SGD(\n",
    "                [torch.tensor([-10.0, -10.0], requires_grad=True, device=cuda)],\n",
    "                lr=0.01,\n",
    "            ),\n",
    "            torch_loss,\n",
    "            DataLoader(dataset=data_set_cuda, batch_size=100),\n",
    "            10000,\n",
    "            torch.optim.lr_scheduler.StepLR,\n",
    "            500,  # Step size\n",
    "            0.5,  # Gamma\n",
    "        ),\n",
    "    },\n",
    "    f_stud,\n",
    "    *data\n",
    ")\n",
    "print(\"Elapsed: \", time.time() - calc_start, \"sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, на предыдущих демонстрациях мы делали epoch*(dataset_size/batch_size)=1000*(100/2)=50000 итераций стохастического спуска.\n",
    "\n",
    "Теперь мы смогли перенести вычисления на GPU, благодаря чему мы можем значительно снизить стоимость увеличения batch_size.\n",
    "То есть теперь мы можем делать не стохастический спуск одновременно по всему датасету, что позволяет значительно повысить точность спуска и, следовательно, увеличить learing_rate и уменьшить число epoch\n",
    "А так же, мы можем бесплатно увеличивать размер датасета, что не повлияет ни на скорость спуска, так как итерации вычисления градиента по всему батчу распараллелены на CUDA ядрах.\n",
    "Это может продолжаться пока у нас хватает вычислителей и видеопамяти для таких манипуляций. То есть теперь batch_size выгодно выставить именно такого размера, какой видеокарта может эффективно распараллелить (в нашем случае датасет имеет размер всего 100, а значит batch_size будет равен размеру датасета).\n",
    "\n",
    "\n",
    "Увеличим epoch в 50 раз, получим 50000*(100/100)=50000 итераций НЕСТОХАСТИЧЕСКОГО спуска. Точность гораздо выше, а время вычислений всего 5 минут (в 5 раз выше однопоточных вычислений на процессоре, при том, что мы увеличили стоимость каждого шага в 50 раз за счёт увеличения batch_size).\n",
    "\n",
    "Теперь воспользуемся преимуществом в точности спуска и уменьшим число epoch в 5 раз (теперь имеем 10000*(100/100)=10000 итераций). Все алгоритмы сошлись идеально в одной точке минимума точнее, чем за 50000 итераций на процессоре (так как спуск более не стохастический всё же), но при этом время вычисления вообще не изменилось: имеем всё те же 60 секунд, что и раньше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе выполнения задания было проведено сравнение собственной реализации методов оптимизации на Python и реализации этих же методов в библиотеке PyTorch. Были рассмотрены следующие методы: SGD, Nesterov, RMSprop, Adagrad и Adam.\n",
    "\n",
    "Согласно полученным результатам, обе реализации показывают схожие результаты в плане сходимости алгоритмов. Однако, реализация PyTorch показала значительное преимущество в скорости работы - она была быстрее на 68%, что является важным фактором при работе с большими объемами данных.\n",
    "\n",
    "Более того, PyTorch предоставляет возможность использования вычислительных ресурсов графического процессора (GPU), что позволяет еще больше ускорить процесс оптимизации. Это особенно актуально при работе с большими объемами данных и когда возможно использование больших размеров батчей.\n",
    "\n",
    "Так, при использовании GPU, время вычислений сократилось до 5 минут при увеличении количества эпох в 50 раз, что в 5 раз быстрее, чем однопоточные вычисления на процессоре. При этом, точность спуска значительно возросла благодаря использованию нестохастического спуска.\n",
    "\n",
    "Вывод: использование готовых реализаций методов оптимизации из библиотеки PyTorch позволяет значительно ускорить процесс оптимизации, увеличить его точность и эффективно использовать ресурсы графического процессора. Это делает PyTorch предпочтительным выбором для работы с большими объемами данных и сложными моделями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Задание 2. Исследование методов оптимизации SciPy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение с прошлой работой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import descent\n",
    "\n",
    "reload(descent)\n",
    "reload(visualization)\n",
    "\n",
    "visualization.visualize_multiple_descent_2args_wh_time(\n",
    "    {\n",
    "        \"bfgs\": lambda: descent.scipy_descent(\n",
    "            f_rosenbrock,\n",
    "            np.array([-10.0, -10.0]),\n",
    "            method=\"bfgs\",\n",
    "            tol=1e-6,\n",
    "            jac=\"2-point\",\n",
    "        ),\n",
    "        \"dogleg\": lambda: descent.scipy_descent(\n",
    "            f_rosenbrock,\n",
    "            np.array([-10.0, -10.0]),\n",
    "            method=\"dogleg\",\n",
    "            tol=1e-6,\n",
    "            jac=descent.get_jac(f_rosenbrock),\n",
    "            hess=descent.get_hess(f_rosenbrock),\n",
    "        ),\n",
    "        \"newton\": lambda: descent.scipy_descent(\n",
    "            f_rosenbrock,\n",
    "            np.array([-10.0, -10.0]),\n",
    "            method=\"newton-cg\",\n",
    "            tol=1e-6,\n",
    "            jac=descent.get_jac(f_rosenbrock),\n",
    "        ),\n",
    "        \"dogleg ours\": lambda: descent.powell_dog_leg(\n",
    "            x0=np.array([-10.0, -10.0]),\n",
    "            rsl=f_rosenbrock_chunk(),\n",
    "            grad=descent.numeric_gradient,\n",
    "        ),\n",
    "        \"bfgs ours\": lambda: bfgs.bfgs(\n",
    "            f_rosenbrock,\n",
    "            descent.numeric_gradient,\n",
    "            x_0=np.array((-1.0, -1.0)),\n",
    "            epochs=1000,\n",
    "        ),\n",
    "        \"l-bfgs ours\": lambda: bfgs.l_bfgs(\n",
    "            f_rosenbrock,\n",
    "            descent.numeric_gradient,\n",
    "            x_0=np.array((-10.0, -10.0)),\n",
    "            epochs=1000,\n",
    "        ),\n",
    "    },\n",
    "    f_rosenbrock,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import descent\n",
    "import bfgs\n",
    "\n",
    "reload(descent)\n",
    "reload(bfgs)\n",
    "reload(visualization)\n",
    "\n",
    "f, f_chunk = dataset.get_linear_loss_func(*data)\n",
    "visualization.linear_multiple_demo_2args_wh_time(\n",
    "    {\n",
    "        \"bfgs\": lambda: descent.scipy_descent(\n",
    "            f, np.array([-10.0, -10.0]), method=\"bfgs\", tol=1e-6, jac=\"2-point\"\n",
    "        ),\n",
    "        \"dogleg\": lambda: descent.scipy_descent(\n",
    "            f,\n",
    "            np.array([-10.0, -10.0]),\n",
    "            method=\"dogleg\",\n",
    "            tol=1e-6,\n",
    "            jac=descent.get_jac(f),\n",
    "            hess=descent.get_hess(f),\n",
    "        ),\n",
    "        \"dogleg ours\": lambda: descent.powell_dog_leg(\n",
    "            x0=np.array([-10.0, -10.0]), rsl=f_chunk, grad=descent.numeric_gradient\n",
    "        ),\n",
    "        \"l-bfgs ours\": lambda: bfgs.l_bfgs(\n",
    "            f, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=10\n",
    "        ),\n",
    "        \"bfgs ours\": lambda: bfgs.bfgs(\n",
    "            f, descent.numeric_gradient, x_0=np.array((5.0, -5.0)), epochs=10\n",
    "        ),\n",
    "    },\n",
    "    f,\n",
    "    *data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy.optimize.least_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление градиента с помощью PyTorch, сравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy.optimize.least_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение параметров и границ, сравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy.optimize.least_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бонусное задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Исследование использования ограничений SciPy при работе с scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy.optimize.LinearConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// :NOTE: Рассмотреть случаи когда минимум находится на границе заданной области и когда он расположен внутри"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy.optimize.NonlinearConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// :NOTE: Рассмотреть случаи когда минимум находится на границе заданной области и когда он расположен внутри"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// вывод"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
